{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Валидация WER на данных LibriSpeech ASR (test-clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед запуском\n",
    "```powershell\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "pip install transformers datasets jiwer psutil\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import io\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from jiwer import wer\n",
    "from jiwer.transforms import Compose, RemovePunctuation, ToLowerCase, ReduceToListOfListOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch dtype: \n",
    "* torch.float32, \n",
    "* torch.float16, \n",
    "* torch.int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Результаты\n",
    "\n",
    "Machine config:\n",
    "\n",
    "**CUDA 12.4, GPU: AD104 12GB VRAM, CPU: 12/24 4.4**\n",
    "\n",
    "Model conf | WER | WER-time | Latency | VRAM | CPU |\n",
    "--- |:---:|:---:|:---:|:---:|:---:|\n",
    "Baseline FP32 (base model, torch.float32)|0.031|12 min 51 sec|x1.00|3.258 GB| 69% |\n",
    "Baseline FP16 (base model, torch.float16)|0.031|9 min 12 sec|x0.72|1.597 GB| 71% |\n",
    "**Quantized Int8** (torch, loaded weights dequantize to fp16)|0.030|8 min 59 sec|x0.70|1.597 GB| 79% |\n",
    "ONNX FP16 (ORTModel (inference only))|0.031|25 min 5 sec|x1.95|73.36 MB| 36% |\n",
    "ONNX QUInt8 (ORTModel + ORTQuantizer + AutoQuantization)|0.032|131 min 04 sec|x10.2|71.32 MB| 95% |\n",
    "ONNX Int8 (Sherpa ONNX)|0.033|149 min 03 sec|x11.6|0.00 MB| 42% |\n",
    "ONNX FP16 (Sherpa ONNX)|0.033|203 min 01 sec|x15.8|0.00 MB| 44% |\n",
    "Faster-Whisper FP16 (Base Inference)|0.145|14 min 26 sec|x1.12|~ 1.900 GB| 7% |\n",
    "Faster-Whisper Int8 (Base Inference)|0.114|13 min 57 sec|x1.09|~ 1.200 GB| 7% |\n",
    "Faster-Whisper FP16 (Batched Inference)|0.039|12 min 50 sec|x1.00|~ 1.800 GB| 8% |\n",
    "Faster-Whisper Int8 (Batched Inference)|0.039|12 min 19 sec|x0.96|~ 1.000 GB| 7% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные конфигурации, которые запустить не удалось (но по карточке модели они могут улучшить показатели):\n",
    "* Использование torch.compile для ускорения в 4,5 раза (невозможность протестировать в jupiter из-за синхронизации логирования, распространенная ошибка в issues модели, решения от разработчиков нет);\n",
    "* Использование Flash Attention 2 (build на windows с учетом рекомендуемых параметров среды занимает более 6 часов, рекомендуется WSL или Linux для запуска) + в версиях torch 2.1.1+ для оптимизации используется Torch Scale-Product-Attention (SDPA); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WER transformation (lowercase + remove punctuation + convert to str words list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_transform = Compose([RemovePunctuation(), ToLowerCase(), ReduceToListOfListOfWords()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_path = \"test-clean.tar.gz\"\n",
    "\n",
    "references = []\n",
    "hypotheses = []\n",
    "\n",
    "total_cpu_time = 0.0\n",
    "total_wall_time = 0.0\n",
    "vram_usages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка test-clean, может использоваться для:\n",
    "* baseline\n",
    "* quantized\n",
    "* optimum\n",
    "    <details><summary>другие</summary>\n",
    "    обработка описана в разделе моделей\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "tar-wer"
    ]
   },
   "outputs": [],
   "source": [
    "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "    # сбор всех транскрипций\n",
    "    transcriptions = {}\n",
    "    for member in tar.getmembers():\n",
    "        if member.name.endswith(\".trans.txt\"):\n",
    "            f = tar.extractfile(member)\n",
    "            content = f.read().decode(\"utf-8\")\n",
    "            for line in content.splitlines():\n",
    "                if line.strip():\n",
    "                    utterance_id, text = line.strip().split(\" \", 1)\n",
    "                    transcriptions[utterance_id] = text\n",
    "\n",
    "    # обработка FLAC записей\n",
    "    for member in tar.getmembers():\n",
    "        if member.name.endswith(\".flac\"):\n",
    "            utterance_id = os.path.splitext(os.path.basename(member.name))[0]\n",
    "            if utterance_id not in transcriptions:\n",
    "                print(f\"Skipping {utterance_id}: No transcript found.\")\n",
    "                continue\n",
    "\n",
    "            f = tar.extractfile(member)\n",
    "            audio_bytes = f.read()\n",
    "            audio_io = io.BytesIO(audio_bytes)\n",
    "            \n",
    "            waveform, sample_rate = torchaudio.load(audio_io)\n",
    "            # mono & resample, если нужно\n",
    "            if waveform.size(0) > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            if sample_rate != 16000:\n",
    "                resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "                waveform = resampler(waveform)\n",
    "            audio_array = waveform.squeeze().numpy().astype(np.float32)\n",
    "\n",
    "            forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n",
    "            inputs = processor(audio_array, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device, dtype=torch_dtype)\n",
    "            \n",
    "            # CPU & wall time перед генеерацией\n",
    "            start_wall = time.time()\n",
    "            process = psutil.Process()\n",
    "            start_cpu = process.cpu_times().user + process.cpu_times().system\n",
    "            \n",
    "            # Сброс CUDA peak memory перед генеерацией\n",
    "            if device.startswith('cuda'):\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                predicted_ids = model.generate(\n",
    "                    inputs,\n",
    "                    forced_decoder_ids=forced_decoder_ids,\n",
    "                    max_length=448\n",
    "                )\n",
    "            \n",
    "            # CPU time & wall time после генерации\n",
    "            end_wall = time.time()\n",
    "            end_cpu = process.cpu_times().user + process.cpu_times().system\n",
    "            \n",
    "            # CPU & wall time accumulate\n",
    "            current_cpu = end_cpu - start_cpu\n",
    "            current_wall = end_wall - start_wall\n",
    "            total_cpu_time += current_cpu\n",
    "            total_wall_time += current_wall\n",
    "            \n",
    "            # VRAM usage\n",
    "            if device.startswith('cuda'):\n",
    "                current_vram = torch.cuda.max_memory_allocated()\n",
    "                vram_usages.append(current_vram)\n",
    "            \n",
    "            transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "            references.append(transcriptions[utterance_id])\n",
    "            hypotheses.append(transcription)\n",
    "            print(f\"Processed: {utterance_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расчет WER и использования ресурсов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if references and hypotheses:\n",
    "    wer_score = wer(\n",
    "        references,\n",
    "        hypotheses,\n",
    "        truth_transform=wer_transform,\n",
    "        hypothesis_transform=wer_transform\n",
    "    )\n",
    "    print(f\"Word Error Rate (WER): {wer_score:.3f}\")\n",
    "    \n",
    "    # VRAM usage\n",
    "    if device.startswith('cuda') and vram_usages:\n",
    "        average_vram = sum(vram_usages) / len(vram_usages)\n",
    "        average_vram_mb = average_vram / (1024 ** 2)\n",
    "        print(f\"VRAM usage during inference: {average_vram_mb:.2f} MB\")\n",
    "    elif not device.startswith('cuda'):\n",
    "        print(\"CPU Inference\")\n",
    "    \n",
    "    # CPU usage\n",
    "    if total_wall_time > 0:\n",
    "        average_cpu_usage_percent = (total_cpu_time / total_wall_time) * 10\n",
    "        print(f\"CPU usage during inference: {average_cpu_usage_percent:.2f}%\")\n",
    "    else:\n",
    "        print(\"No CPU data\")\n",
    "    \n",
    "else:\n",
    "    print(\"Распознаваний не обнаружено.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FP16/FP32 CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выбора 16 или 32 - необходимо поставить соответсвующее значение в 4 ячейке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model pipeline\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    return_timestamps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model parameter dtype: {next(model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Pipe parameter dtype: {pipe.torch_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing and WER calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "js_code = \"\"\"\n",
    "var cells = Jupyter.notebook.get_cells();\n",
    "for (var i = 0; i < cells.length; i++) {\n",
    "    if (cells[i].metadata.tags && cells[i].metadata.tags.includes(\"target_cell\")) {\n",
    "        cells[i].execute();\n",
    "        break;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "html = f'<a href=\"#\" onclick=\"{js_code}\">Click to Run Target Cell</a>'\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch copmile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n",
    ").to(device)\n",
    "\n",
    "# Enable static cache and compile the forward pass\n",
    "model.generation_config.cache_implementation = \"static\"\n",
    "model.generation_config.max_new_tokens = 256\n",
    "model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    return_timestamps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "# # 2 warmup steps\n",
    "# for _ in tqdm(range(2), desc=\"Warm-up step\"):\n",
    "#     with sdpa_kernel(SDPBackend.MATH):\n",
    "#         result = pipe(sample.copy(), generate_kwargs={\"min_new_tokens\": 256, \"max_new_tokens\": 256})\n",
    "\n",
    "# fast run\n",
    "with sdpa_kernel(SDPBackend.MATH):\n",
    "    result = pipe(sample.copy())\n",
    "\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error: here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-Quantized Int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Квантизация модели описана в файле quantization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, pipeline, AutoModelForSpeechSeq2Seq\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedWhisperForSpeechSeq2Seq(AutoModelForSpeechSeq2Seq):\n",
    "    @classmethod\n",
    "    def from_pretrained_quantized(cls, pretrained_model_name_or_path, quantized_checkpoint_path):\n",
    "        \"\"\"\n",
    "        Загрузка base model через from_pretrained() и перезапись весов модели\n",
    "        деквантованными весами из checkpoint.\n",
    "        Checkpoint это словарь с ключами:\n",
    "          - \"state_dict\": словарь, отображающий имена параметров в тензоры int8\n",
    "          - \"scales\": словарь, отображающий имена параметров в scale factor.\n",
    "        \"\"\"\n",
    "        # quantized checkpoint\n",
    "        checkpoint = torch.load(quantized_checkpoint_path, map_location=\"cpu\")\n",
    "        quantized_state_dict = checkpoint[\"state_dict\"]\n",
    "        scales = checkpoint[\"scales\"]\n",
    "\n",
    "        # оригинальная модель с HF (FP16)\n",
    "        model = cls.from_pretrained(pretrained_model_name_or_path, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)      \n",
    "        # новый словарь состояний, деквантуя каждый параметр\n",
    "        new_state_dict = {}\n",
    "        for name, param in quantized_state_dict.items():\n",
    "            if name in scales:\n",
    "                # преобразование int8 tensor в float16 умножение на его scale factor\n",
    "                new_state_dict[name] = param.to(torch.float16) * scales[name]\n",
    "            else:\n",
    "                new_state_dict[name] = param\n",
    "\n",
    "        # деквантованный словарь состояний в модель\n",
    "        # strict=False если ключи отсутствуют/не совпадают\n",
    "        model.load_state_dict(new_state_dict, strict=False)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "quantized_checkpoint_path = \"whisper_large_v3_turbo_int8.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = QuantizedWhisperForSpeechSeq2Seq.from_pretrained_quantized(\n",
    "    model_id, quantized_checkpoint_path\n",
    ")\n",
    "# quantized_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=quantized_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    return_timestamps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model parameter dtype: {next(quantized_model.parameters()).dtype}\", f\"Pipeline dtype: {pipe.torch_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = quantized_model\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-ONNX with Optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конвертация модели в onnx описана в файле onnx.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед использованием - рекомендация для CUDAExecutionProvider\n",
    "```powershell\n",
    "pip uninstall onnx\n",
    "pip uninstall onnxruntime\n",
    "pip install optimum[onnxruntime-gpu]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_v1 = 'onnxf/whisper_lv3t_onnx_v1' #QUInt8\n",
    "onnx_v2 = 'onnxf/whisper_lv3t_onnx_v2' #FP16\n",
    "onnx_v3 = 'onnxf/whisper_lv3t_onnx_v3' #FP16 optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSpeechSeq2Seq\n",
    "from transformers import pipeline, AutoTokenizer, AutoFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onnxmodel(path):\n",
    "    model = ORTModelForSpeechSeq2Seq.from_pretrained(path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(path)\n",
    "    return model, tokenizer, feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, feature_extractor = onnxmodel(onnx_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"automatic-speech-recognition\", model=model, tokenizer=tokenizer, feature_extractor=feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Pipeline dtype: {pipe.torch_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONNX with Sherpa-onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед использованием:\n",
    "* посмотреть версии [моделей](https://k2-fsa.github.io/sherpa/onnx/pretrained_models/whisper/export-onnx.html#available-models);\n",
    "* склонировать репозиторий выбранной модели;\n",
    "* установить инструменты с помощью\n",
    "\n",
    "    ```powershell\n",
    "    pip install torch openai-whisper onnxruntime onnx librosa soundfile\n",
    "\n",
    "    git clone https://github.com/k2-fsa/sherpa-onnx/\n",
    "    cd sherpa-onnx/scripts/whisper\n",
    "    ```\n",
    "\n",
    "* и далее конвертация в onnx, либо тестирование с помощью\n",
    "\n",
    "    ```powershell\n",
    "    python3 ./export-onnx.py --model modelName\n",
    "    ```\n",
    "\n",
    "* или\n",
    "\n",
    "    ```powershell\n",
    "    python3 ./test.py \\\n",
    "    --encoder ./modelName-encoder.onnx \\\n",
    "    --decoder ./modelName-decoder.onnx \\\n",
    "    --tokens ./modelName-tokens.txt \\\n",
    "    ./path-to-audio.wav\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оригинальные [файлы](https://github.com/k2-fsa/sherpa-onnx/tree/master/scripts/whisper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скрипт запуска модели из консоли (powershell tested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./sherpa_onnx_test.py --encoder ../sherpa-onnx-whisper-turbo/turbo-encoder.onnx --decoder ../sherpa-onnx-whisper-turbo/turbo-decoder.onnx --tokens ../sherpa-onnx-whisper-turbo/turbo-tokens.txt ./test_wavs-0.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмуляция запуска через sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = './test_wavs-0.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--encoder', '../sherpa-onnx-whisper-turbo/turbo-encoder.onnx',\n",
    "    '--decoder', '../sherpa-onnx-whisper-turbo/turbo-decoder.onnx',\n",
    "    '--tokens', '../sherpa-onnx-whisper-turbo/turbo-tokens.txt',\n",
    "    test\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sherpa_onnx_test\n",
    "result_list = sherpa_onnx_test.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(result_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперименты с тестированием модели:\n",
    "* рекомендация к использованию - распаковать архив (не удаляя tar.gz) и запустить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sherpa_onnx_test\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "    # сбор всех транскрипций\n",
    "    transcriptions = {}\n",
    "    for member in tar.getmembers():\n",
    "        if member.name.endswith(\".trans.txt\"):\n",
    "            f = tar.extractfile(member)\n",
    "            content = f.read().decode(\"utf-8\")\n",
    "            for line in content.splitlines():\n",
    "                if line.strip():\n",
    "                    utterance_id, text = line.strip().split(\" \", 1)\n",
    "                    transcriptions[utterance_id] = text\n",
    "\n",
    "    # обработка FLAC записей\n",
    "    for member in tar.getmembers():\n",
    "        if k == 10: break\n",
    "        if member.name.endswith(\".flac\"):\n",
    "            utterance_id = os.path.splitext(os.path.basename(member.name))[0]\n",
    "            if utterance_id not in transcriptions:\n",
    "                print(f\"Skipping {utterance_id}: No transcript found.\")\n",
    "                continue\n",
    "            \n",
    "            # CPU & wall time перед генеерацией\n",
    "            start_wall = time.time()\n",
    "            process = psutil.Process()\n",
    "            start_cpu = process.cpu_times().user + process.cpu_times().system\n",
    "            \n",
    "            # Сброс CUDA peak memory перед генеерацией\n",
    "            if device.startswith('cuda'):\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            # CPU time & wall time после генерации\n",
    "            end_wall = time.time()\n",
    "            end_cpu = process.cpu_times().user + process.cpu_times().system\n",
    "            \n",
    "            # CPU & wall time accumulate\n",
    "            current_cpu = end_cpu - start_cpu\n",
    "            current_wall = end_wall - start_wall\n",
    "            total_cpu_time += current_cpu\n",
    "            total_wall_time += current_wall\n",
    "            \n",
    "            # VRAM usage\n",
    "            if device.startswith('cuda'):\n",
    "                current_vram = torch.cuda.max_memory_allocated()\n",
    "                vram_usages.append(current_vram)\n",
    "            \n",
    "            filename='test-clean/'+f'{member.name}'\n",
    "            sys.argv = [\n",
    "                'test.py',\n",
    "                '--encoder', '../sherpa-onnx-whisper-turbo/turbo-encoder.int8.onnx',\n",
    "                '--decoder', '../sherpa-onnx-whisper-turbo/turbo-decoder.int8.onnx',\n",
    "                '--tokens', '../sherpa-onnx-whisper-turbo/turbo-tokens.txt',\n",
    "                filename\n",
    "            ]\n",
    "            \n",
    "            transcription = str(sherpa_onnx_test.main())\n",
    "\n",
    "            references.append(transcriptions[utterance_id])\n",
    "            hypotheses.append(transcription)\n",
    "            print(f\"Processed: {utterance_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faster-Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед запуском\n",
    "```powershell\n",
    "pip install faster-whisper\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel, BatchedInferencePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"large-v3-turbo\"\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запуск на GPU с FP16\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "\n",
    "# или запуск на GPU с INT8\n",
    "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
    "# или запуск на CPU с INT8\n",
    "# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, info = model.transcribe(\"test_wavs-0.wav\", beam_size=5)\n",
    "\n",
    "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "for segment in segments:\n",
    "    print((segment.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batched Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_model = BatchedInferencePipeline(model=model)\n",
    "segments, info = batched_model.transcribe(\"test_wavs-0.wav\", batch_size=16)\n",
    "\n",
    "for segment in segments:\n",
    "    print((segment.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперимент с тестированием модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "    # сбор всех транскрипций\n",
    "    transcriptions = {}\n",
    "    for member in tar.getmembers():\n",
    "        if member.name.endswith(\".trans.txt\"):\n",
    "            f = tar.extractfile(member)\n",
    "            content = f.read().decode(\"utf-8\")\n",
    "            for line in content.splitlines():\n",
    "                if line.strip():\n",
    "                    utterance_id, text = line.strip().split(\" \", 1)\n",
    "                    transcriptions[utterance_id] = text\n",
    "\n",
    "    # обработка FLAC записей\n",
    "    for member in tar.getmembers():\n",
    "        # if k == 10: break\n",
    "        if member.name.endswith(\".flac\"):\n",
    "            utterance_id = os.path.splitext(os.path.basename(member.name))[0]\n",
    "            if utterance_id not in transcriptions:\n",
    "                print(f\"Skipping {utterance_id}: No transcript found.\")\n",
    "                continue\n",
    "\n",
    "            f = tar.extractfile(member)\n",
    "            audio_bytes = f.read()\n",
    "            audio_io = io.BytesIO(audio_bytes)\n",
    "            \n",
    "            waveform, sample_rate = torchaudio.load(audio_io)\n",
    "            # mono & resample, если нужно\n",
    "            if waveform.size(0) > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            if sample_rate != 16000:\n",
    "                resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "                waveform = resampler(waveform)\n",
    "            audio_array = waveform.squeeze().numpy().astype(np.float32)\n",
    "            \n",
    "            # CPU & wall time перед генеерацией\n",
    "            start_wall = time.time()\n",
    "            process = psutil.Process()\n",
    "            start_cpu = process.cpu_times().user + process.cpu_times().system\n",
    "            \n",
    "            # Сброс CUDA peak memory перед генеерацией\n",
    "            if device.startswith('cuda'):\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            '''batched inference'''\n",
    "            # batched_model = BatchedInferencePipeline(model=model)\n",
    "            # segments, info = batched_model.transcribe(audio_array, batch_size=batch_size)\n",
    "            \n",
    "            '''base inference'''\n",
    "            segments, info = model.transcribe(audio_array, beam_size=5)\n",
    "\n",
    "            # CPU time & wall time после генерации\n",
    "            end_wall = time.time()\n",
    "            end_cpu = process.cpu_times().user + process.cpu_times().system\n",
    "            \n",
    "            # CPU & wall time accumulate\n",
    "            current_cpu = end_cpu - start_cpu\n",
    "            current_wall = end_wall - start_wall\n",
    "            total_cpu_time += current_cpu\n",
    "            total_wall_time += current_wall\n",
    "            \n",
    "            # VRAM usage\n",
    "            if device.startswith('cuda'):\n",
    "                current_vram = torch.cuda.max_memory_allocated()\n",
    "                vram_usages.append(current_vram)\n",
    "            \n",
    "            for segment in segments:\n",
    "                transcription = segment.text\n",
    "\n",
    "            references.append(transcriptions[utterance_id])\n",
    "            hypotheses.append(transcription)\n",
    "            print(f\"Processed: {utterance_id}\")\n",
    "            k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
